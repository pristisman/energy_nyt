---
title: "Exploratory Data Analysis"
format: pdf
editor: visual
autor: Priscila Stisman
---

## Quarto

```{r}
###Setup
library(tidyverse)
library(dplyr)
library(here)
library(stringr)
library(ggplot2)
library(quanteda)
library(quanteda.textplots)
library(vader)
library(tidymodels)
library(topicmodels)
library(tidytext)
```

```{r}
### Read data
nyt <- read_csv(here("data", "nyt_combined_2021_2023.csv")) %>%
  mutate(pub_date = ymd_hms(pub_date),  # ensure it's a datetime
         post_war = if_else(pub_date >= as.POSIXct("2022-02-24 00:00:00"), 1, 0)) %>%
  select(-date_parsed, -keywords)
```

### Filter news by words

```{r}
### Filter news by the word usage related to energy

nyt <- nyt %>%
  mutate(
    headline = str_to_lower(headline),
    abstract = str_to_lower(abstract),
    lead_paragraph = str_to_lower(lead_paragraph)
  ) #I select all news articles that use any word from my set of energy keywords either in the headline, abstract or lead paragraph


energy_keywords <- c(
  "energy", "climate", "climate change", "global warming", "coal", "wind",
  "solar", "nuclear", "biofuels", "gas", "gasoline", "natural gas", "oil", "fossil fuels",
  "renewable", "fuel", "hydropower", "electricity", "power grid", "emissions", "carbon",
  "greenhouse gases", "decarbonization", "clean energy", "transition", "sustainability", "green economy", "methane", "hydrogen", "infrastructure", "lithium", "electric vehicles", "clean technology", "COP26", "Paris Agreement", "environment", "net zero", "footprint", "IPCC", "UNFCCC", "greentech", "petroleum", "ecological", "environmental", "climate resilience", "EIA", "biofuel", "bio", "twh", "terawatt", "kilowatt", "flood", "hurricane", "earthquake", "greenhouse", "lithium", "pipeline", "gazprom", "nord stream 1", "nord stream", "methane", "cng", "lng", "wti price", "brent crude", "crude oil","liquefied", "minerals", "clean energy", "heat", "warming", "blackout", "opec")

# Create regex pattern
pattern <- str_c(energy_keywords, collapse = "|") 

# Filter dataset
energy_nyt <- nyt %>%
  filter(str_detect(headline, pattern) |
         str_detect(abstract, pattern) |
           str_detect(lead_paragraph, pattern))
  

energy_nyt <- energy_nyt %>%
  mutate(year = year(pub_date),
headline = str_match(energy_nyt$headline, "'main': '([^']+)'")[,2])
  

```

### Pre-processing

```{r}
# Pre-processing
set.seed(2710)
#1.1 Creating corpus from the lead paragraph
corpus <- corpus(energy_nyt, text_field = "lead_paragraph") 

#1.2 Tokenize

tokens <- tokens(corpus, 
                 split_hyphens = FALSE, # keep hyphenated words
                 remove_punct = TRUE, # remove punctuation
                 remove_numbers = TRUE, # remove digits
                 remove_symbols = TRUE, # remove symbols
                 valuetype = "regex") |>  
  tokens_tolower() |> 
  tokens_wordstem() |> 
  tokens_remove(c(stopwords("en"), 
                          "said", "articl", "first", "two", "one", "like", 
                          "new", "day", "presid", "peopl", "time", "theater", 
                          "year", "week", "citi", "it’"))



dfm <- tokens |> 
        dfm()

dfm <- dfm |> dfm(min_docfreq = 0.05, max_docfreq = 0.95, docfreq_type = "prop", verbose = TRUE) 
```

### Word Clouds by year (TODO ESTO ES LEAD_PARAGRAPH)

```{r}

## Word Cloud for 2021
png("wordcloud_2021.png", width = 800, height = 600, res = 150)

textplot_wordcloud(dfm_subset(dfm, year==2021),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("Word Cloud from Lead Paragraph, 2021")

dev.off()

```

```{r}

## Word cloud for 2022

png("wordcloud_2022.png", width = 800, height = 600, res = 150)

textplot_wordcloud(dfm_subset(dfm, year==2022),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("Word Cloud from Lead Paragraph, 2022")


dev.off()

```

```{r}
## Word Cloud for 2023

png("wordcloud_2023.png", width = 800, height = 600, res = 150)

textplot_wordcloud(dfm_subset(dfm, year==2023),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("Word Cloud from Lead Paragraph, 2023")

dev.off()
```

## Top Words LEADPARAGRAPH

```{r}
set.seed(2710)
# 2021
top_features_2021 <- topfeatures(dfm_subset(dfm, year==2021), n=10)
print("Top 10 words of 2021:")
print(top_features_2021)

# Para 2022
top_features_2022 <- topfeatures(dfm_subset(dfm, year==2022), n=10)
print("Top 10 words of 2022:")
print(top_features_2022)

# Para 2023
top_features_2023 <- topfeatures(dfm_subset(dfm, year==2023), n=10)
print("Top 10 words of 2023:")
print(top_features_2023)
```

### Sentiment Analysis related to energy policy

```{r}
### Filter news by the word usage related to energy


policy_keywords <- c("policy", "congress", "bill", "congressional", "government", "administration", "authority", "agency", "ministry", "department", "bureau", "commission", "executive", "federal", "state", "municipal", "parliament", "government", "administration", "authority", "agency", "ministry", "department", "bureau", "commission", "executive", "federal", "state", "municipal", "parliament", "implementation", "enforcement", "compliance", "draft", "debate", "hearing", "vote", "negotiation", "policy-making", "consultation", "lawmaker", "legislator", "senator", "representative", "policy-maker", "policyholder", "stakeholder", "lobby", "interest group")

# Create regex pattern
policy_pattern <- str_c(policy_keywords, collapse = "|")

# Filter dataset
energy_policy_nyt <- energy_nyt %>%
  filter(str_detect(headline, policy_pattern) |
         str_detect(abstract, policy_pattern) |
           str_detect(lead_paragraph, policy_pattern))


```

```{r}
## SENTIMENT ANALYSIS DE HEADLINES

sentiment_analysis_policy <- energy_policy_nyt |>
  mutate(sentiment_score = sapply(headline, function(x) get_vader(x)[["compound"]])) 
#This function applies the VADER sentiment analysis to each tweet (x). 
#It returns the compound score from the sentiment analysis result. 

#The compound score is a single metric that summarizes the overall sentiment, ranging from -1 (most negative) to +1 (most positive).

sentiment_analysis_policy <- sentiment_analysis_policy |> 
  select(abstract, year, post_war, sentiment_score) |>
  mutate(sentiment = case_when(sentiment_score>0 ~ "positive",
                               sentiment_score == 0 ~ "neutral",
                               sentiment_score<0 ~ "negative"))


sentiment_percent_war_policy <- sentiment_analysis_policy %>%
  count(post_war, sentiment) %>%
  group_by(post_war) %>%
  mutate(percentage = n / sum(n) * 100) %>%
  ungroup()


ggplot(sentiment_percent_war_policy, aes(x = post_war, y = percentage, fill = sentiment)) +
  geom_col(position = "stack") +
  scale_fill_manual(
    name = "Sentiment", 
    values = c(negative = "brown2", 
               positive = "darkolivegreen2", 
               neutral = "darkslategray3"),
    labels = c(negative = "Negative",
               neutral = "Neutral",
               positive = "Positive"),
      na.translate = FALSE
  ) +
  scale_x_continuous(breaks = c(0, 1), labels = c("Pre-war", "Post-war")) +
  labs(
    title = "Sentiment in NYT Headlines Before and After the Russia-Ukraine War",
    subtitle = "Distribution of positive, neutral, and negative headlines related to energy policy",
    x = "",
    y = "Percentage"
  ) +
  theme_minimal()

ggsave("sentiment_analysis_plot.png")

```

## Topic Modeling pre and post war with all energy related words, not only policy related issues

```{r}
#prewar
dfm_prewar <- dfm_subset(dfm, post_war==0)

set.seed(2710)
dfm_prewar_lda <- convert(dfm_prewar, to="topicmodels") 
k = 5
lda_prewar <- LDA(dfm_prewar_lda, k = k, control = list(seed = 1234))

#Explore Betas
topics_prewar <- tidy(lda_prewar, matrix = "beta")
head(topics_prewar)

# Visualize the Top Words in Each Topic  
top_terms_prewar <- topics_prewar %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# Plot top words per topic
top_terms_prewar %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic Pre-War", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()

#ggsave("topic_modeling_prewar.png")


```

```{r}
#postwar

dfm_postwar <- dfm_subset(dfm, post_war==1)

set.seed(2710)
dfm_postwar_lda <- convert(dfm_postwar, to="topicmodels") 
k = 5
lda_postwar <- LDA(dfm_postwar_lda, k = k, control = list(seed = 1234))

#Explore Betas
topics_postwar <- tidy(lda_postwar, matrix = "beta")
head(topics)

# Visualize the Top Words in Each Topic  
top_terms_postwar <- topics_postwar %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# Plot top words per topic
top_terms_postwar %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic Post-War ", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()

#ggsave("topic_modeling_postwar.png")


```

### Optimal K for Topic Modeling

```{r}
## COHERENCE SCORE TO CHOOSE OPTIMAL K

# Define a range of k values to test
k_values <- 2:25
coherence_values <- numeric(length(k_values))

# Function to calculate a simple coherence measure
calculate_coherence <- function(lda_model) {
  # Extract top terms
  topics <- tidytext::tidy(lda_model, matrix = "beta")
  
  # Get top terms by topic and calculate coherence
  coherence <- topics %>%
    group_by(topic) %>%
    slice_max(beta, n = 10) %>%
    summarize(topic_coherence = mean(beta)) %>%
    summarize(avg_coherence = mean(topic_coherence)) %>%
    pull(avg_coherence)
  
  return(coherence)
}

# Compute coherence for each k
set.seed(2025)
for (i in seq_along(k_values)) {
  cat("Fitting LDA model with", k_values[i], "topics...\n")
  lda_model <- LDA(dfm_prewar_lda, k = k_values[i], control = list(seed = 2025))
  coherence_values[i] <- calculate_coherence(lda_model)
}

# Create dataframe for plotting
elbow_coherence_df <- data.frame(k = k_values, coherence = coherence_values)


ggplot(elbow_coherence_df, aes(x = k, y = coherence)) +
  geom_line() + 
  geom_point() +
  labs(
    title = "Coherence Scores for Different Numbers of Topics",
    subtitle = "Based on pre-war data only",
    x = "Number of topics (k)", 
    y = "Coherence Score" # higher is better
  ) +
  theme_minimal()

ggsave("coherence_plot.png")


##LOCAL MAXIMUM AT 5 - WORTH PAYING ATTENTION
## Consider a compromise position like k=20, which shows improved coherence over lower values but remains manageable for interpretation.
## Manually inspect the topics at several key points (perhaps k=5, k=16, and k=25) to assess their interpretability and usefulness for your specific research question.

```

```{r}

## PERPLEXITY SCORE

# Define a range of k (number of topics) values to test

perplexity_values <- numeric(length(k_values))

# Compute perplexity for each k
set.seed(2710)
for (i in seq_along(k_values)) {
  cat("Fitting LDA model with", k_values[i], "topics...\n")
  lda_model <- LDA(dfm_postwar_lda, k = k_values[i], control = list(seed = 2025))
  perplexity_values[i] <- perplexity(lda_model)
}

# Create dataframe for plotting
elbow_perplexity_df <- data.frame(k = k_values, perplexity = perplexity_values)

# Plot the elbow curve
ggplot(elbow_perplexity_df, aes(x = k, y = perplexity)) +
  geom_line() + 
  geom_point() +
  labs(title = "Elbow Method for Optimal Number of Topics",
       subtitle = "Based on post-war data only",
       x = "Number of topics (k)", 
       y = "Perplexity") + #lower is better
  theme_minimal()

ggsave("perplexity_plot.png")


# If simplicity is important, consider k=2, but be aware that this might be oversimplifying your data into too few topics.
# If you want more granular topics with both good perplexity and coherence, aim for k=23-25.
# Since both metrics favor higher k values (23-25), this is likely the sweet spot that balances model fit and topic differentiation.

```

## Regression Analysis



```{r}

energy_nyt <- energy_nyt %>%
  mutate(time = as.numeric(as.Date(pub_date) - as.Date("2022-02-24")),
         doc_id = row_number())
 


# Combine all three text fields for tokenization
nyt_energy_tokens <- energy_nyt %>%
  # Reshape data to have one row per text field per document
  pivot_longer(
    cols = c(headline, abstract, lead_paragraph),
    names_to = "text_field",
    values_to = "text"
  ) %>%
  # Remove any NA values
  filter(!is.na(text)) %>%
  # Select just what we need for tokenization
  select(doc_id, text_field, text) %>%
  # Tokenize all text
  unnest_tokens(word, text)

# Calculate energy metrics
energy_counts <- nyt_energy_tokens %>%
  mutate(is_energy = word %in% energy_keywords) %>%
  group_by(doc_id) %>%
  summarise(
    energy_score = sum(is_energy),
    total_words = n(),
    energy_prop = (energy_score / total_words)*100
  )

# Join back to main dataset
energy_reg <- energy_nyt %>%
  left_join(energy_counts, by = "doc_id")

```



```{r}
# Find the rows where energy score is 0
# zero_score_articles <- energy_nyt %>%
#   filter(energy_score == 0)
# 
# # Check where the keywords appear in these articles
# zero_score_checks <- zero_score_articles %>%
#   mutate(
#     headline_match = str_extract_all(headline, pattern),
#     abstract_match = str_extract_all(abstract, pattern),
#     lead_match = str_extract_all(lead_paragraph, pattern)
#   )


# Remove rows with energy_score = 0 before regression
energy_reg <- energy_reg %>%
  filter(energy_score > 0)

```





```{r}
#regression #1
model1 <- lm(energy_prop ~ post_war, data = energy_reg)
summary(model1)
```

```{r}
# Regression #2
model3 <- lm(energy_prop ~ time + post_war + time:post_war, data = energy_reg)
summary(model3)

```

## STM?

```{r}

library(stm)
set.seed(2710)

# Prepare corpus and metadata

processed <- textProcessor(documents = energy_reg$lead_paragraph, 
                           metadata = energy_reg %>% 
                            select(post_war, time))



out <- prepDocuments(processed$documents, processed$vocab, processed$meta)

# Fit STM with covariates
stm_model <- stm(documents = out$documents, 
                vocab = out$vocab,
                K = 5, 
                prevalence = ~ post_war + time + post_war:time,
                max.em.its = 75,
                data = out$meta,
                seed = 2710)


# Examine effects
topic_effects <- estimateEffect(formula = ~ post_war + time + post_war:time, 
                              stmobj = stm_model,
                              metadata = out$meta)


labelTopics(stm_model)


# Topic 1 Top Words:
#  	 Highest Prob: new, energi, like, week, countri, first, even 
#  	 FREX: carbon, energi, inflat, russian, look, forc, start 
#  	 Lift: porsch, cleanser, hatch, club’, vitamin, “core”, bridge” 
#  	 Score: new, energi, like, week, countri, russian, first 
# Topic 2 Top Words:
#  	 Highest Prob: climat, presid, unit, washington, nuclear, nation, hous 
#  	 FREX: biden’, republican, presid, tax, democrat, washington, polici 
#  	 Lift: academia, ayatollah, capito, litani, shelley, convection”, fratern 
#  	 Score: climat, presid, washington, unit, nation, nuclear, hous 
# Topic 3 Top Words:
#  	 Highest Prob: time, home, can, wednesday, hurrican, tuesday, govern 
#  	 FREX: room, grill, recip, ice, meat, ingredi, get 
#  	 Lift: “best, aqsa, burnish, carey, coldwel, danielgalecom, fiercest 
#  	 Score: time, can, home, hurrican, get, wednesday, made 
# Topic 4 Top Words:
#  	 Highest Prob: year, one, chang, oil, day, ukrain, will 
#  	 FREX: oil, will, fuel, day, one, world’, environment 
#  	 Lift: bulki, burnt, herbac, prawn, stephenson, morcilla, neal 
#  	 Score: year, oil, chang, one, will, ukrain, day 
# Topic 5 Top Words:
#  	 Highest Prob: state, said, gas, peopl, price, thursday, month 
#  	 FREX: price, peopl, high, electr, rain, thursday, area 
#  	 Lift: diaz, bedtim, cold”, aaa, toad, “unfriend, countries” 
#  	 Score: state, gas, said, peopl, price, thursday, electr 





# topic_labels <- c("Topic 1: Energy Crisis", #eneri appears in each list
#                   "Topic 2: US Politics", 
#                   "Topic 3: Everyday Life", 
#                   "Topic 4: Fossil Fuels", 
#                   "Topic 5: Gas Price")




# War Effect
plot(topic_effects, covariate = "post_war", topics = 1:5, 
     model = stm_model, method = "difference", 
     cov.value1 = 1, cov.value2 = 0)



# Time trends
plot(topic_effects, covariate = "time", topics = 1:5, 
     model = stm_model, method = "continuous")




# Interaction effect (changes in time trends after war)
plot(topic_effects, covariate = "time", topics = 1:5, 
     model = stm_model, method = "continuous", 
     moderator = "post_war", moderator.value = 1)



# Using regression splines for time variable
library(splines)

topic_effects <- estimateEffect(1:5 ~ post_war + ns(time, df = 5),
                                stmobj = stm_model,
                                metadata = out$meta,
                                uncertainty = "Global")

labelTopics(stm_model)


plot(topic_effects,
     covariate = "time",
     method = "continuous",
     topics = 1:5,
     model = stm_model,
     printlegend = TRUE)




```



