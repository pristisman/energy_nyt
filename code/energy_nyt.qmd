---
title: "Energy Policy in Flux: An NLP Approach to Media Discourse Before and After the Russia-Ukraine Conflict"
format: pdf
editor: visual
autor: Priscila Stisman
embed-resources: true
---

```{r}
###Setup
library(tidyverse)
library(dplyr)
library(here)
library(stringr)
library(ggplot2)
library(quanteda)
library(quanteda.textplots)
library(vader)
library(tidymodels)
library(topicmodels)
library(tidytext)
library(stm)
library(splines)
```

## Building Corpus

```{r}
### Read data
nyt <- read_csv(here("data", "nyt_combined_2021_2023.csv")) %>%
  mutate(pub_date = ymd_hms(pub_date),  # ensure it's a datetime
         post_war = if_else(pub_date >= as.POSIXct("2022-02-24 00:00:00"), 1, 0)) %>%
  select(-date_parsed, -keywords)
```

```{r}
### Filter news by the word usage related to energy

nyt <- nyt %>%
  mutate(
    headline = str_to_lower(headline),
    abstract = str_to_lower(abstract),
    lead_paragraph = str_to_lower(lead_paragraph)
  ) #I select all news articles that use any word from my set of energy keywords either in the headline, abstract or lead paragraph


energy_keywords <- c(
  "energy", "climate", "climate change", "global warming", "coal", "wind",
  "solar", "nuclear", "biofuels", "gas", "gasoline", "natural gas", "oil", "fossil fuels",
  "renewable", "fuel", "hydropower", "electricity", "power grid", "emissions", "carbon",
  "greenhouse gases", "decarbonization", "clean energy", "transition", "sustainability", "green economy", "methane", "hydrogen", "infrastructure", "lithium", "electric vehicles", "clean technology", "COP26", "Paris Agreement", "environment", "net zero", "footprint", "IPCC", "UNFCCC", "greentech", "petroleum", "ecological", "environmental", "climate resilience", "EIA", "biofuel", "bio", "twh", "terawatt", "kilowatt", "flood", "hurricane", "earthquake", "greenhouse", "lithium", "pipeline", "gazprom", "nord stream 1", "nord stream", "methane", "cng", "lng", "wti price", "brent crude", "crude oil","liquefied", "minerals", "clean energy", "heat", "warming", "blackout", "opec")

# Create regex pattern
pattern <- str_c(energy_keywords, collapse = "|") 

# Filter dataset
energy_nyt <- nyt %>%
  filter(str_detect(headline, pattern) |
         str_detect(abstract, pattern) |
           str_detect(lead_paragraph, pattern))
  

energy_nyt <- energy_nyt %>%
  mutate(year = year(pub_date),
headline = str_match(energy_nyt$headline, "'main': '([^']+)'")[,2])
  

```

## Pre-processing

The document-feature matrix is constructed using the lead paragraphs of energy-related New York Times articles published between 2021 and 2023.

```{r}
# Setting seed for reproducibility
set.seed(2710)

#1.1 Creating corpus from the lead paragraph
corpus <- corpus(energy_nyt, text_field = "lead_paragraph") 

#1.2 Tokenize

tokens <- tokens(corpus, 
                 split_hyphens = FALSE, # keep hyphenated words
                 remove_punct = TRUE, # remove punctuation
                 remove_numbers = TRUE, # remove digits
                 remove_symbols = TRUE, # remove symbols
                 valuetype = "regex") |>  
  tokens_tolower() |> 
  tokens_wordstem() |> 
  tokens_remove(c(stopwords("en"), 
                          "said", "articl", "first", "two", "one", "like", 
                          "new", "day", "presid", "peopl", "time", "theater", 
                          "year", "week", "citi", "it’", "york", "unit", "can", "last", "home", "month", "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday", "week"))

# 1.3 Document Feature Matrix

dfm <- tokens |> 
        dfm()

dfm <- dfm |> dfm(min_docfreq = 0.05, max_docfreq = 0.95, docfreq_type = "prop", verbose = TRUE) 
```

## Exploratory Data Analysis

### Word Clouds by Year

Word clouds are generated by year based on lead paragraphs.

```{r}
## Word Cloud for 2021
#Uses lead paragraph

set.seed(2710)

#png("wordcloud_2021.png", width = 800, height = 600, res = 150)

textplot_wordcloud(dfm_subset(dfm, year==2021),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("")

#dev.off()

```

```{r}
## Word cloud for 2022
#Uses lead paragraph

set.seed(2710)

#png("wordcloud_2022.png", width = 800, height = 600, res = 150)

textplot_wordcloud(dfm_subset(dfm, year==2022),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("")

#dev.off()


```

```{r}
## Word Cloud for 2023
#Uses lead paragraph

set.seed(2710)

#png("wordcloud_2023.png", width = 800, height = 600, res = 150)

textplot_wordcloud(dfm_subset(dfm, year==2023),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("")

#dev.off()

```

### Top Words

Top words are generated by year based on lead paragraphs.

```{r}
#Uses lead paragraph
set.seed(2710)

# 2021
top_features_2021 <- topfeatures(dfm_subset(dfm, year==2021), n=10)
print("Top 10 words of 2021:")
print(top_features_2021)

# Para 2022
top_features_2022 <- topfeatures(dfm_subset(dfm, year==2022), n=10)
print("Top 10 words of 2022:")
print(top_features_2022)

# Para 2023
top_features_2023 <- topfeatures(dfm_subset(dfm, year==2023), n=10)
print("Top 10 words of 2023:")
print(top_features_2023)
```

## Sentiment Analysis - Energy Policy Articles

I conduct a sentiment analysis using VADER to examine changes in tone across energy-policy-related articles published before and after the onset of the war.

### Building energy-policy related articles

```{r}
### Filter news by the word usage related to energy


policy_keywords <- c("policy", "congress", "bill", "congressional", "government", "administration", "authority", "agency", "ministry", "department", "bureau", "commission", "executive", "federal", "state", "municipal", "parliament", "government", "administration", "authority", "agency", "ministry", "department", "bureau", "commission", "executive", "federal", "state", "municipal", "parliament", "implementation", "enforcement", "compliance", "draft", "debate", "hearing", "vote", "negotiation", "policy-making", "consultation", "lawmaker", "legislator", "senator", "representative", "policy-maker", "policyholder", "stakeholder", "lobby", "interest group")

# Create regex pattern
policy_pattern <- str_c(policy_keywords, collapse = "|")

# Filter dataset
energy_policy_nyt <- energy_nyt %>%
  filter(str_detect(headline, policy_pattern) |
         str_detect(abstract, policy_pattern) |
           str_detect(lead_paragraph, policy_pattern))


```

```{r}
## Analyzing headlines sentiment pre and post war

sentiment_analysis_policy <- energy_policy_nyt |>
  mutate(sentiment_score = sapply(headline, function(x) get_vader(x)[["compound"]])) 

#This function applies the VADER sentiment analysis to each headline (x). 
#It returns the compound score from the sentiment analysis result. 

#The compound score is a single metric that summarizes the overall sentiment, ranging from -1 (most negative) to +1 (most positive).

sentiment_analysis_policy <- sentiment_analysis_policy |> 
  select(headline, year, post_war, sentiment_score) |>
  mutate(sentiment = case_when(sentiment_score>0 ~ "positive",
                               sentiment_score == 0 ~ "neutral",
                               sentiment_score<0 ~ "negative"))


sentiment_percent_war_policy <- sentiment_analysis_policy %>%
  count(post_war, sentiment) %>%
  group_by(post_war) %>%
  mutate(percentage = n / sum(n) * 100) %>%
  ungroup()


ggplot(sentiment_percent_war_policy, aes(x = post_war, y = percentage, fill = sentiment)) +
  geom_col(position = "stack") +
  scale_fill_manual(
    name = "", 
    values = c(negative = "brown2", 
               positive = "darkolivegreen2", 
               neutral = "darkslategray3"),
    labels = c(negative = "Negative",
               neutral = "Neutral",
               positive = "Positive"),
      na.translate = FALSE
  ) +
  scale_x_continuous(breaks = c(0, 1), labels = c("Pre-war", "Post-war")) +
  labs(
    title = "Sentiment in NYT Headlines Before and After the Russia-Ukraine War",
    subtitle = "Distribution of positive, neutral, and negative headlines related to energy policy",
    x = "",
    y = "Percentage"
  ) +
  theme_minimal()

#ggsave("sentiment_analysis_plot.png")

```

## Topic Modeling

### Latent Dirichlet Allocation (LDA)

#### LDA pre and post war

I apply Latent Dirichlet Allocation (LDA) to energy-related articles before and after the war to identify dominant topics.

```{r}
#prewar
dfm_prewar <- dfm_subset(dfm, post_war==0)

set.seed(2710)
dfm_prewar_lda <- convert(dfm_prewar, to="topicmodels") 
k = 5
lda_prewar <- LDA(dfm_prewar_lda, k = k, control = list(seed = 1234))

#Explore Betas
topics_prewar <- tidy(lda_prewar, matrix = "beta")
head(topics_prewar)

# Visualize the Top Words in Each Topic  
top_terms_prewar <- topics_prewar %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# Plot top words per topic
top_terms_prewar %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic Pre-War", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()


## ADDING LABELS

prewar_topic_labels <- c("Climate Policy", 
                  "US Politics", 
                  "Climate Change", 
                  "US Pandemic Response", 
                  "Post-Pandemic Recovery")

# Create a new topic variable with the labels
top_terms_prewar_modified <- top_terms_prewar %>%
  mutate(topic_label = factor(topic, 
                             levels = 1:5, 
                             labels = prewar_topic_labels))

# Use the new topic_label variable for faceting
top_terms_prewar_modified %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic_label, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic Pre-War", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()




#ggsave("topic_modeling_prewar_labels.png")

```

```{r}
#postwar

dfm_postwar <- dfm_subset(dfm, post_war==1)

set.seed(2710)
dfm_postwar_lda <- convert(dfm_postwar, to="topicmodels") 
k = 5
lda_postwar <- LDA(dfm_postwar_lda, k = k, control = list(seed = 1234))

#Explore Betas
topics_postwar <- tidy(lda_postwar, matrix = "beta")
head(topics)

# Visualize the Top Words in Each Topic  
top_terms_postwar <- topics_postwar %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# Plot top words per topic
top_terms_postwar %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic Post-War ", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()


## ADDING LABELS

postwar_topic_labels <- c("Nuclear Energy", 
                  "US Foreign Policy", 
                  "Climate Change and War", 
                  "Gas and Oil", 
                 "Climate Crisis")

# Create a new topic variable with the labels
top_terms_postwar_modified <- top_terms_postwar %>%
  mutate(topic_label = factor(topic, 
                             levels = 1:5, 
                             labels = postwar_topic_labels))

# Use the new topic_label variable for faceting
top_terms_postwar_modified %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic_label, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic Post-War", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()



#ggsave("topic_modeling_postwar_labels.png")


```

#### LDA by year

I apply Latent Dirichlet Allocation (LDA) to energy-related articles by year to identify dominant topics.

```{r}

dfm_2021 <- dfm_subset(dfm, year==2021)

set.seed(2710)
dfm_2021_lda <- convert(dfm_2021, to="topicmodels") 
k = 5
lda_2021 <- LDA(dfm_2021_lda, k = k, control = list(seed = 1234))

#Explore Betas
topics_2021 <- tidy(lda_2021, matrix = "beta")
head(topics)

# Visualize the Top Words in Each Topic  
top_terms_2021 <- topics_2021 %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# Plot top words per topic
top_terms_2021 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic 2021 ", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()

## ADDING LABELS
topic_labels_2021 <- c("Local Governance", 
                  "US Politics", 
                  "Climate Change and Pandemic", 
                  "Health Policy", 
                 "Pandemic Response")

# Create a new topic variable with the labels
top_terms_2021_modified <- top_terms_2021 %>%
  mutate(topic_label = factor(topic, 
                             levels = 1:5, 
                             labels = topic_labels_2021))

# Use the new topic_label variable for faceting
top_terms_2021_modified %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic_label, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic 2021", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()





#ggsave("topic_modeling_2021.png")

```

```{r}

dfm_2022 <- dfm_subset(dfm, year==2022)

set.seed(2710)
dfm_2022_lda <- convert(dfm_2022, to="topicmodels") 
k = 5
lda_2022 <- LDA(dfm_2022_lda, k = k, control = list(seed = 1234))

#Explore Betas
topics_2022 <- tidy(lda_2022, matrix = "beta")
head(topics)

# Visualize the Top Words in Each Topic  
top_terms_2022 <- topics_2022 %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# Plot top words per topic
top_terms_2022 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic 2022 ", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()

## ADDING LABELS
topic_labels_2022 <- c(
  "Nuclear Energy", 
  "Energy Crisis and Political Response", 
  "Oil Market", 
  "War & Gas", 
  "Energy Prices and Climate Impact"
)

# Create a new topic variable with the labels
top_terms_2022_modified <- top_terms_2022 %>%
  mutate(topic_label = factor(topic, 
                             levels = 1:5, 
                             labels = topic_labels_2022))

# Use the new topic_label variable for faceting
top_terms_2022_modified %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic_label, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic 2022", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()




#ggsave("topic_modeling_2022.png")

```

```{r}

dfm_2023 <- dfm_subset(dfm, year==2023)

set.seed(2710)
dfm_2023_lda <- convert(dfm_2023, to="topicmodels") 
k = 5
lda_2023 <- LDA(dfm_2023_lda, k = k, control = list(seed = 1234))

#Explore Betas
topics_2023 <- tidy(lda_2023, matrix = "beta")
head(topics)

# Visualize the Top Words in Each Topic  
top_terms_2023 <- topics_2023 %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# Plot top words per topic
top_terms_2023 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic 2023 ", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()


## ADDING LABELS
topic_labels_2023 <- c(
  "Games", 
  "Entertainment & Leisure", 
  "US Energy Policy", 
  "Home & Living Spaces", 
  "Lifestyle"
)

# Create a new topic variable with the labels
top_terms_2023_modified <- top_terms_2023 %>%
  mutate(topic_label = factor(topic, 
                             levels = 1:5, 
                             labels = topic_labels_2023))

# Use the new topic_label variable for faceting
top_terms_2023_modified %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic_label, scales = "free") +
  scale_y_reordered() +
  labs(title = "Top Words in Each Topic 2023", x = "Word Probability (Beta)", y = "Word") +
  theme_minimal()



#ggsave("topic_modeling_2023.png")

```

#### Optimal K for Topic Modeling

#### Coherence Score

```{r}

# Define a range of k values to test
k_values <- 2:25
coherence_values <- numeric(length(k_values))

# Function to calculate a simple coherence measure
calculate_coherence <- function(lda_model) {
  # Extract top terms
  topics <- tidytext::tidy(lda_model, matrix = "beta")
  
  # Get top terms by topic and calculate coherence
  coherence <- topics %>%
    group_by(topic) %>%
    slice_max(beta, n = 10) %>%
    summarize(topic_coherence = mean(beta)) %>%
    summarize(avg_coherence = mean(topic_coherence)) %>%
    pull(avg_coherence)
  
  return(coherence)
}

# Compute coherence for each k
set.seed(2025)
for (i in seq_along(k_values)) {
  cat("Fitting LDA model with", k_values[i], "topics...\n")
  lda_model <- LDA(dfm_prewar_lda, k = k_values[i], control = list(seed = 2025))
  coherence_values[i] <- calculate_coherence(lda_model)
}

# Create dataframe for plotting
elbow_coherence_df <- data.frame(k = k_values, coherence = coherence_values)


ggplot(elbow_coherence_df, aes(x = k, y = coherence)) +
  geom_line() + 
  geom_point() +
  labs(
    title = "Coherence Scores for Different Numbers of Topics",
    subtitle = "Based on pre-war data only",
    x = "Number of topics (k)", 
    y = "Coherence Score" # higher is better
  ) +
  theme_minimal()

ggsave("coherence_plot.png")


##LOCAL MAXIMUM AT 5 - WORTH PAYING ATTENTION
## Consider a compromise position like k=20, which shows improved coherence over lower values but remains manageable for interpretation.
## Manually inspect the topics at several key points (perhaps k=5, k=16, and k=25) to assess their interpretability and usefulness for your specific research question.

```

#### Perplexity Score

```{r}

# Define a range of k (number of topics) values to test

perplexity_values <- numeric(length(k_values))

# Compute perplexity for each k
set.seed(2710)
for (i in seq_along(k_values)) {
  cat("Fitting LDA model with", k_values[i], "topics...\n")
  lda_model <- LDA(dfm_postwar_lda, k = k_values[i], control = list(seed = 2025))
  perplexity_values[i] <- perplexity(lda_model)
}

# Create dataframe for plotting
elbow_perplexity_df <- data.frame(k = k_values, perplexity = perplexity_values)

# Plot the elbow curve
ggplot(elbow_perplexity_df, aes(x = k, y = perplexity)) +
  geom_line() + 
  geom_point() +
  labs(title = "Elbow Method for Optimal Number of Topics",
       subtitle = "Based on post-war data only",
       x = "Number of topics (k)", 
       y = "Perplexity") + #lower is better
  theme_minimal()

#ggsave("perplexity_plot.png")


```

## Regression Analysis

I conduct an interrupted time series analysis, with the onset of the war on February 24, 2022, serving as the intervention point. The energy score is calculated as the proportion of energy-related keywords found in the headline, abstract, or lead paragraph relative to the total word count.

```{r}
energy_nyt <- energy_nyt %>%
  mutate(time = as.numeric(as.Date(pub_date) - as.Date("2022-02-24")),
         doc_id = row_number())


# Combine all three text fields for tokenization
nyt_energy_tokens <- energy_nyt %>%
  # Reshape data to have one row per text field per document
  pivot_longer(
    cols = c(headline, abstract, lead_paragraph),
    names_to = "text_field",
    values_to = "text"
  ) %>%
  # Remove any NA values
  filter(!is.na(text)) %>%
  # Select just what we need for tokenization
  select(doc_id, text_field, text) %>%
  # Tokenize all text
  unnest_tokens(word, text)

# Calculate energy metrics
energy_counts <- nyt_energy_tokens %>%
  mutate(is_energy = word %in% energy_keywords) %>%
  group_by(doc_id) %>%
  summarise(
    energy_score = sum(is_energy),
    total_words = n(),
    energy_prop = (energy_score / total_words)*100
  )

# Join back to main dataset
energy_reg <- energy_nyt %>%
  left_join(energy_counts, by = "doc_id")

```

```{r}
# Find the rows where energy score is 0
# zero_score_articles <- energy_nyt %>%
#   filter(energy_score == 0)
# 
# # Check where the keywords appear in these articles
# zero_score_checks <- zero_score_articles %>%
#   mutate(
#     headline_match = str_extract_all(headline, pattern),
#     abstract_match = str_extract_all(abstract, pattern),
#     lead_match = str_extract_all(lead_paragraph, pattern)
#   )


# Remove rows with energy_score = 0 before regression
energy_reg <- energy_reg %>%
  filter(energy_score > 0)

```

### Model 1: no trend

```{r}
model1 <- lm(energy_prop ~ post_war, data = energy_reg)
summary(model1)
```

### Model 1: with trend

```{r}
model2 <- lm(energy_prop ~ time + post_war + time:post_war, data = energy_reg)
summary(model2)
```

### Chow Test

```{r}
# Split the data into pre-war and post-war periods
pre_war_data <- energy_reg %>% filter(post_war == 0)
post_war_data <- energy_reg %>% filter(post_war == 1)

# Run separate regressions for each period
model_pre <- lm(energy_prop ~ time, data = pre_war_data)
model_post <- lm(energy_prop ~ time, data = post_war_data)

# Run regression for the pooled data
model_pooled <- lm(energy_prop ~ time, data = energy_reg)

# Calculate RSS (Residual Sum of Squares) for each model
rss_pre <- sum(model_pre$residuals^2)
rss_post <- sum(model_post$residuals^2)
rss_pooled <- sum(model_pooled$residuals^2)

# Calculate n1, n2 (sample sizes) and k (number of parameters)
n1 <- nrow(pre_war_data)
n2 <- nrow(post_war_data)
k <- length(model_pre$coefficients)  # Number of parameters in each equation

# Calculate the Chow test statistic
chow_statistic <- ((rss_pooled - (rss_pre + rss_post)) / k) / 
                  ((rss_pre + rss_post) / (n1 + n2 - 2*k))

# Calculate p-value
p_value <- 1 - pf(chow_statistic, k, n1 + n2 - 2*k)

# Print results
cat("Chow Test Statistic:", chow_statistic, "\n")
cat("P-value:", p_value, "\n")
cat("Reject null hypothesis of no structural break:", p_value < 0.05, "\n")
```

## Structural Topic Modeling

I conduct Structural Topic Modeling (STM) using the same corpus as in the regression analysis above. The text input consists of lead paragraphs, and the model includes covariates for the war intervention, a time trend, and their interaction.

### Model

```{r}
set.seed(2710)

# Prepare corpus and metadata

processed <- textProcessor(documents = energy_reg$lead_paragraph, 
                           metadata = energy_reg %>% 
                            select(post_war, time))

out <- prepDocuments(processed$documents, processed$vocab, processed$meta)

# Fit STM with covariates
stm_model <- stm(documents = out$documents, 
                vocab = out$vocab,
                K = 5, 
                prevalence = ~ post_war + time + post_war:time,
                max.em.its = 75,
                data = out$meta,
                seed = 2710)


# Examine effects
topic_effects <- estimateEffect(formula = ~ post_war + time + post_war:time, 
                              stmobj = stm_model,
                              metadata = out$meta)


labelTopics(stm_model)
labelTopics(stm_model, n = 20) # to see top 20 words


# Topic 1 Top Words:
#  	 Highest Prob: new, energi, like, week, countri, first, even 
#  	 FREX: carbon, energi, inflat, russian, look, forc, start 
#  	 Lift: porsch, cleanser, hatch, club’, vitamin, “core”, bridge” 
#  	 Score: new, energi, like, week, countri, russian, first 
# Topic 2 Top Words:
#  	 Highest Prob: climat, presid, unit, washington, nuclear, nation, hous 
#  	 FREX: biden’, republican, presid, tax, democrat, washington, polici 
#  	 Lift: academia, ayatollah, capito, litani, shelley, convection”, fratern 
#  	 Score: climat, presid, washington, unit, nation, nuclear, hous 
# Topic 3 Top Words:
#  	 Highest Prob: time, home, can, wednesday, hurrican, tuesday, govern 
#  	 FREX: room, grill, recip, ice, meat, ingredi, get 
#  	 Lift: “best, aqsa, burnish, carey, coldwel, danielgalecom, fiercest 
#  	 Score: time, can, home, hurrican, get, wednesday, made 
# Topic 4 Top Words:
#  	 Highest Prob: year, one, chang, oil, day, ukrain, will 
#  	 FREX: oil, will, fuel, day, one, world’, environment 
#  	 Lift: bulki, burnt, herbac, prawn, stephenson, morcilla, neal 
#  	 Score: year, oil, chang, one, will, ukrain, day 
# Topic 5 Top Words:
#  	 Highest Prob: state, said, gas, peopl, price, thursday, month 
#  	 FREX: price, peopl, high, electr, rain, thursday, area 
#  	 Lift: diaz, bedtim, cold”, aaa, toad, “unfriend, countries” 
#  	 Score: state, gas, said, peopl, price, thursday, electr 


topic_labels <- c("Topic 1: Greenhouse Emissions", 
                  "Topic 2: US Politics", 
                  "Topic 3: Lifestyle", 
                  "Topic 4: Energy Crisis", 
                 "Topic 5: Gas Price")
```

### Plots

```{r}
##### War Effect #####

#png("war_effect.png", width = 800, height = 600, res = 150)

plot(topic_effects, 
     covariate = "post_war", 
     topics = 1:5, 
     model = stm_model, 
     method = "difference", 
     cov.value1 = 1, 
     cov.value2 = 0,
     labeltype = "custom",
     custom.labels = topic_labels)

#dev.off()


##### Time trends #####

plot(topic_effects, covariate = "time", topics = 1:5, 
     model = stm_model, method = "continuous")


##### Interaction effect  #####
plot(topic_effects, covariate = "time", topics = 1:5, 
     model = stm_model, method = "continuous", 
     moderator = "post_war", moderator.value = 1)

```

### Using Regression Splines for nonlinearities

```{r}
### Using regression splines for time variable - allows for nonlinearities
# A natural spline with 5 degrees of freedom to model non-linear effects of time.

topic_effects <- estimateEffect(1:5 ~ post_war + ns(time, df = 5), 
                                stmobj = stm_model,
                                metadata = out$meta,
                                uncertainty = "Global")

labelTopics(stm_model)


#png("stm_time.png", width = 800, height = 600, res = 150)

##### Time trend

plot(topic_effects,
     covariate = "time",
     method = "continuous",
     topics = 1:5,
     model = stm_model,
     printlegend = TRUE,
     #labeltype = "custom",
     custom.labels = topic_labels)

title(xlab = "Days from War Start Date")


#dev.off()
```
