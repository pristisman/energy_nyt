---
title: "exploratory data analysis"
format: pdf
editor: visual
---

```{r}
###Setup
library(tidyverse)
library(dplyr)
library(data.table)
library(here)
library(stringr)
library(ggplot2)
library(quanteda)
library(quanteda.textplots)

```

```{r}
### Read data
nyt <- read_csv(here("data", "nyt_combined_2021_2023.csv")) %>%
  mutate(pub_date = ymd_hms(pub_date),  # ensure it's a datetime
         post_war = if_else(pub_date >= as.POSIXct("2022-02-24 00:00:00"), 1, 0)) %>%
  select(-date_parsed, -keywords)
```

### Filter news by words

```{r}
### Filter news by the word usage related to energy

nyt <- nyt %>%
  mutate(
    headline = str_to_lower(headline),
    abstract = str_to_lower(abstract),
    lead_paragraph = str_to_lower(lead_paragraph)
  ) #I select all news articles that use any word from my set of energy keywords either in the headline, abstract or lead paragraph


energy_keywords <- c(
  "energy", "climate", "climate change", "global warming", "coal", "wind",
  "solar", "nuclear", "biofuels", "gas", "gasoline", "natural gas", "oil", "fossil fuels",
  "renewable", "fuel", "hydropower", "electricity", "power grid", "emissions", "carbon",
  "greenhouse gases", "decarbonization", "clean energy", "transition", "sustainability", "green economy", "methane", "hydrogen", "infrastructure", "lithium", "electric vehicles", "clean technology", "COP26", "Paris Agreement", "environment", "net zero", "footprint", "IPCC", "UNFCCC", "greentech", "petroleum", "ecological", "environmental", "climate resilience", "EIA", "biofuel", "bio", "twh", "terawatt", "kilowatt", "flood", "hurricane", "earthquake", "greenhouse", "lithium", "pipeline", "gazprom", "nord stream 1", "nord stream", "methane", "cng", "lng", "wti price", "brent crude", "crude oil","liquefied", "minerals", "clean energy", "heat", "warming", "blackout", "opec")

# Create regex pattern
pattern <- str_c(energy_keywords, collapse = "|") 

# Filter dataset
energy_nyt <- nyt %>%
  filter(str_detect(headline, pattern) |
         str_detect(abstract, pattern) |
           str_detect(lead_paragraph, pattern))
  

energy_nyt <- energy_nyt %>%
  mutate(year = year(pub_date))

energy_nyt <- energy_nyt %>% 
  mutate(headline = str_match(energy_nyt$headline, "'main': '([^']+)'")[,2])
  

```


### Pre-processing

```{r}
# Pre-processing
set.seed(2710)
#1.1 Creating corpus from the lead paragraph
corpus <- corpus(energy_nyt, text_field = "lead_paragraph") 

#1.2 Tokenize

tokens <- tokens(corpus, 
                 split_hyphens = FALSE, # keep hyphenated words
                 remove_punct = TRUE, # remove punctuation
                 remove_numbers = TRUE, # remove digits
                 remove_symbols = TRUE, # remove symbols
                 valuetype = "regex") |>  
  tokens_tolower() |> 
  tokens_wordstem() |> 
  tokens_remove(c(stopwords("en"), 
                          "said", "articl", "first", "two", "one", "like", 
                          "new", "day", "presid", "peopl", "time", "theater", 
                          "year", "week", "citi", "it’"))



dfm <- tokens |> 
        dfm()

dfm <- dfm |> dfm(min_docfreq = 0.05, max_docfreq = 0.95, docfreq_type = "prop", verbose = TRUE) 

topfeatures(dfm)
```

### Word Clouds pre and post war

```{r}

## Word cloud pre war

textplot_wordcloud(dfm_subset(dfm, post_war==0),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("Word Cloud pre war")

top_features_prewar <- topfeatures(dfm_subset(dfm, post_war==0), n=10)
print(top_features_prewar)


textplot_wordcloud(dfm_subset(dfm, post_war==1),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("Word Cloud post war")

top_features_postwar <- topfeatures(dfm_subset(dfm, post_war==1), n=10)
print(top_features_postwar)

```


### Word Clouds by YEAR

```{r}

## Word cloud for the Republican party in 2021

textplot_wordcloud(dfm_subset(dfm, year==2021),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("Word Cloud for 2021")

```

```{r}

## Word cloud for the Republican party in 2022

textplot_wordcloud(dfm_subset(dfm, year==2022),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("Word Cloud for 2022")

```

```{r}
## Word cloud for the Republican party in 2023

textplot_wordcloud(dfm_subset(dfm, year==2023),
                   random_order = FALSE, 
                   rotation = 0.25, 
                   max_words = 100, 
                   min_size = 0.5, 
                   max_size = 2.8, 
                   color = RColorBrewer::brewer.pal(8, "Set3"))  
title("Word Cloud for 2023")

```

### Top Words - Withouth TFIDF

```{r}
set.seed(2710)
# Para mostrar las palabras más frecuentes de 2021
top_features_2021 <- topfeatures(dfm_subset(dfm, year==2021), n=10)
print("Top 20 palabras de 2021:")
print(top_features_2021)

# Para 2022
top_features_2022 <- topfeatures(dfm_subset(dfm, year==2022), n=10)
print("Top 20 palabras de 2022:")
print(top_features_2022)

# Para 2023
top_features_2023 <- topfeatures(dfm_subset(dfm, year==2023), n=10)
print("Top 20 palabras de 2023:")
print(top_features_2023)
```

### Top Words - TFIDF

```{r}
# 
# # Para 2021 (con TF-IDF aplicado)
# dfm_tfidf_2021 <- dfm_subset(dfm, year==2021) |>
#   dfm_tfidf(scheme_tf = "prop", scheme_df = "inversemax")
# top_features_tfidf_2021 <- topfeatures(dfm_tfidf_2021, n=10)
# print("Top 10 palabras de 2021 por TF-IDF:")
# print(top_features_tfidf_2021)
# 
# # Para 2022
# dfm_tfidf_2022 <- dfm_subset(dfm, year==2022) |>
#   dfm_tfidf(scheme_tf = "prop", scheme_df = "inversemax")
# top_features_tfidf_2022 <- topfeatures(dfm_tfidf_2022, n=10)
# print("Top 10 palabras de 2022 por TF-IDF:")
# print(top_features_tfidf_2022)
# 
# # Para 2023
# dfm_tfidf_2023 <- dfm_subset(dfm, year==2023) |>
#   dfm_tfidf(scheme_tf = "prop", scheme_df = "inversemax")
# top_features_tfidf_2023 <- topfeatures(dfm_tfidf_2023, n=10)
# print("Top 10 palabras de 2023 por TF-IDF:")
# print(top_features_tfidf_2023)


```



### Dictionary
#Sentiment Analysis for headlines

```{r}
library(vader) #suitable? yes

sentiment_analysis <- energy_nyt |>
  mutate(sentiment_score = sapply(headline, function(x) get_vader(x)[["compound"]])) 
#This function applies the VADER sentiment analysis to each tweet (x). 
#It returns the compound score from the sentiment analysis result. 

#The compound score is a single metric that summarizes the overall sentiment, ranging from -1 (most negative) to +1 (most positive).

sentiment_analysis <- sentiment_analysis |> 
  select(abstract, year, post_war, sentiment_score) |>
  mutate(sentiment = case_when(sentiment_score>0 ~ "positive",
                               sentiment_score == 0 ~ "neutral",
                               sentiment_score<0 ~ "negative"))

# Agrupar por año y sentimiento, y calcular el porcentaje de cada sentimiento por año
sentiment_percent_by_year <- sentiment_analysis %>%
  count(year, sentiment) %>%
  group_by(year) %>%
  mutate(percentage = n / sum(n) * 100) %>%
  ungroup()

# Gráfico de barras con distribución de sentimientos por año
ggplot(sentiment_percent_by_year, aes(x = year, y = percentage, fill = sentiment)) +
  geom_col(position = "stack") +
  scale_fill_manual(name = "Sentiment", 
                    values = c(negative = "brown2", 
                               positive = "darkolivegreen2", 
                               neutral = "darkslategray3"),
                    labels = c(negative = "Negative",
                               neutral = "Neutral",
                               positive = "Positive")) +
  labs(title = "Sentiment Distribution by Year",
       x = "Year",
       y = "Percentage") +
  theme_minimal() +
  theme(legend.position = "none")

```



## FALTA REPLICAR LO MISMO PRE WAR Y POST WAR

```{r}



sentiment_percent_war <- sentiment_analysis %>%
  count(post_war, sentiment) %>%
  group_by(post_war) %>%
  mutate(percentage = n / sum(n) * 100) %>%
  ungroup()

# Gráfico de barras con distribución de sentimientos por año
ggplot(sentiment_percent_war, aes(x = post_war, y = percentage, fill = sentiment)) +
  geom_col(position = "stack") +
  scale_fill_manual(name = "Sentiment", 
                    values = c(negative = "brown2", 
                               positive = "darkolivegreen2", 
                               neutral = "darkslategray3"),
                    labels = c(negative = "Negative",
                               neutral = "Neutral",
                               positive = "Positive")) +
  scale_x_continuous(breaks = c(0, 1), labels = c("Pre-war", "Post-war")) +
  labs(title = "Sentiment Distribution by Year",
       x = "",
       y = "Percentage") +
  theme_minimal() +
  theme(legend.position = "none")



```

